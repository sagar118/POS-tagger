{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  3914\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "print('Length of the dataset: ', len(nltk_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3914 sentences in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first two sentences in the corpus\n",
    "nltk_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train set:  3718\n",
      "Length of Test set:  196\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "random.seed(100)\n",
    "train_set, test_set = train_test_split(nltk_data, test_size=0.05)\n",
    "\n",
    "print('Length of Train set: ', len(train_set))\n",
    "print('Length of Test set: ', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train tagged set:  95635\n",
      "Length of Test tagged set:  5041\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of lists to a single list of all the words\n",
    "train_tagged_words = [train_pair for sent in train_set for train_pair in sent]\n",
    "test_tagged_words = [test_pair for sent in test_set for test_pair in sent]\n",
    "\n",
    "print('Length of Train tagged set: ', len(train_tagged_words))\n",
    "print('Length of Test tagged set: ', len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  12108\n",
      "Number of unique tags:  12\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the number of unique words and tags in train set\n",
    "vocab = set([tup[0] for tup in train_tagged_words])\n",
    "tags = set([tup[1] for tup in train_tagged_words])\n",
    "\n",
    "print('Number of unique words: ', len(vocab))\n",
    "print('Number of unique tags: ', len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at the tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging Algorithm - HMM\n",
    "\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n",
    "\n",
    "In other words, to every word w, assign the tag t that maximises the likelihood P(t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to compute P(w/t) and P(t).\n",
    "\n",
    "\n",
    "P(w/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n",
    "\n",
    "P(w/t) = count(w, t) / count(t). \n",
    "\n",
    "\n",
    "The term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n",
    "\n",
    "Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P(t) and store them in two large matrices. The matrix of P(w/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    tag_count = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    w_given_tag_count = len(w_given_tag_list)\n",
    "    \n",
    "    return (w_given_tag_count, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " large\n",
      "(29, 6062)\n",
      "(0, 12872)\n",
      "(0, 27460) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "# large\n",
    "print(\"\\n\", \"large\")\n",
    "print(word_given_tag('large', 'ADJ'))\n",
    "print(word_given_tag('large', 'VERB'))\n",
    "print(word_given_tag('large', 'NOUN'), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    t1_count = len([t for t in tags if t==t1])\n",
    "    t2_t1_count = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1]==t2:\n",
    "            t2_t1_count += 1\n",
    "    \n",
    "    return (t2_t1_count, t1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6062)\n",
      "(4243, 6062)\n",
      "(5314, 8282)\n",
      "(460, 12872)\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "print(t2_given_t1(t2='PRON', t1='ADJ'))\n",
    "print(t2_given_t1('NOUN', 'ADJ'))\n",
    "print(t2_given_t1('NOUN', 'DET'))\n",
    "print(t2_given_t1('PRON', 'VERB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>DET</th>\n",
       "      <th>X</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>.</th>\n",
       "      <th>ADP</th>\n",
       "      <th>VERB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.057629</td>\n",
       "      <td>0.099869</td>\n",
       "      <td>0.013098</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.245252</td>\n",
       "      <td>0.042567</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.401768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.094961</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>0.210078</td>\n",
       "      <td>0.042248</td>\n",
       "      <td>0.023643</td>\n",
       "      <td>0.481783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.039981</td>\n",
       "      <td>0.122269</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.054858</td>\n",
       "      <td>0.119944</td>\n",
       "      <td>0.348675</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>0.156206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>0.185561</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.210322</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>0.351432</td>\n",
       "      <td>0.117542</td>\n",
       "      <td>0.035203</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.641632</td>\n",
       "      <td>0.016783</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.184613</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>0.073909</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.016566</td>\n",
       "      <td>0.060847</td>\n",
       "      <td>0.164224</td>\n",
       "      <td>0.144154</td>\n",
       "      <td>0.205161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.032375</td>\n",
       "      <td>0.066733</td>\n",
       "      <td>0.023125</td>\n",
       "      <td>0.079617</td>\n",
       "      <td>0.130492</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.135117</td>\n",
       "      <td>0.117278</td>\n",
       "      <td>0.347539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.016826</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.066315</td>\n",
       "      <td>0.699934</td>\n",
       "      <td>0.064995</td>\n",
       "      <td>0.078027</td>\n",
       "      <td>0.012372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.043955</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>0.029097</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>0.264457</td>\n",
       "      <td>0.239803</td>\n",
       "      <td>0.176621</td>\n",
       "      <td>0.147123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.065046</td>\n",
       "      <td>0.057599</td>\n",
       "      <td>0.080836</td>\n",
       "      <td>0.173067</td>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.052934</td>\n",
       "      <td>0.043872</td>\n",
       "      <td>0.223488</td>\n",
       "      <td>0.092858</td>\n",
       "      <td>0.091154</td>\n",
       "      <td>0.089001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.068723</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.061360</td>\n",
       "      <td>0.323765</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.107459</td>\n",
       "      <td>0.323018</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.016861</td>\n",
       "      <td>0.008644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.031153</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.134245</td>\n",
       "      <td>0.217371</td>\n",
       "      <td>0.082427</td>\n",
       "      <td>0.064792</td>\n",
       "      <td>0.110783</td>\n",
       "      <td>0.035970</td>\n",
       "      <td>0.091983</td>\n",
       "      <td>0.167418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PRT      PRON      CONJ       NUM       DET         X       ADV  \\\n",
       "PRT   0.001965  0.017682  0.002292  0.057629  0.099869  0.013098  0.010151   \n",
       "PRON  0.011628  0.008140  0.005039  0.006589  0.009690  0.094961  0.033333   \n",
       "CONJ  0.004184  0.056253  0.000465  0.039981  0.122269  0.008833  0.054858   \n",
       "NUM   0.027745  0.001492  0.014021  0.185561  0.003580  0.210322  0.002685   \n",
       "DET   0.000241  0.003260  0.000483  0.022217  0.005192  0.046245  0.012437   \n",
       "X     0.184613  0.055272  0.010672  0.002867  0.055272  0.073909  0.026442   \n",
       "ADV   0.014536  0.014536  0.006938  0.032375  0.066733  0.023125  0.079617   \n",
       "ADJ   0.010558  0.000660  0.016826  0.019795  0.004619  0.021115  0.004784   \n",
       "NOUN  0.043955  0.004661  0.042717  0.009468  0.013292  0.029097  0.016970   \n",
       ".     0.002422  0.065046  0.057599  0.080836  0.173067  0.027633  0.052934   \n",
       "ADP   0.001281  0.068723  0.000747  0.061360  0.323765  0.034041  0.013552   \n",
       "VERB  0.031153  0.035736  0.005205  0.022918  0.134245  0.217371  0.082427   \n",
       "\n",
       "           ADJ      NOUN         .       ADP      VERB  \n",
       "PRT   0.087099  0.245252  0.042567  0.020629  0.401768  \n",
       "PRON  0.072868  0.210078  0.042248  0.023643  0.481783  \n",
       "CONJ  0.119944  0.348675  0.035797  0.052534  0.156206  \n",
       "NUM   0.032518  0.351432  0.117542  0.035203  0.017900  \n",
       "DET   0.203453  0.641632  0.016783  0.009056  0.039000  \n",
       "X     0.016566  0.060847  0.164224  0.144154  0.205161  \n",
       "ADV   0.130492  0.031715  0.135117  0.117278  0.347539  \n",
       "ADJ   0.066315  0.699934  0.064995  0.078027  0.012372  \n",
       "NOUN  0.011835  0.264457  0.239803  0.176621  0.147123  \n",
       ".     0.043872  0.223488  0.092858  0.091154  0.089001  \n",
       "ADP   0.107459  0.323018  0.040551  0.016861  0.008644  \n",
       "VERB  0.064792  0.110783  0.035970  0.091983  0.167418  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "Let's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag/w)=P(w/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag/w).\n",
    "\n",
    "We'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n",
    "\n",
    "Note: P(tag|start) = P(tag|'.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)\n",
    "# print(tagged_seq)\n",
    "# print(test_run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_tagged_words) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(check)/len(tagged_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases = [[test_tagged_words[i-1],j] for i, j in enumerate(zip(tagged_seq, test_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
