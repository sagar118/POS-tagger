{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBubELzIaonE"
   },
   "source": [
    "# POS Tagging\n",
    "A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ef3Dzg2oezFE"
   },
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpaDMS6dezFK"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOg2xVglezFN"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import re, time, nltk, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UepDxZ6HezFY",
    "outputId": "2bd14f3e-6a31-4fe7-9241-6fba8d4bc3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  3914\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "print('Length of the dataset: ', len(nltk_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lN8nTL5eezFk"
   },
   "source": [
    "There are 3914 sentences in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "wg0YK3f7ezFm",
    "outputId": "7ac3083e-bd2b-4df1-8329-3e196e2c774a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first two sentences in the corpus\n",
    "nltk_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Im89jY-gezFv",
    "outputId": "bd803e89-c004-45e3-84e1-a64898fe4771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train set:  3718\n",
      "Length of Validation set:  196\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and validation\n",
    "# Train - 95%\n",
    "# Validation - 5%\n",
    "\n",
    "random.seed(100)\n",
    "train_set, validation_set = train_test_split(nltk_data, test_size=0.05)\n",
    "\n",
    "print('Length of Train set: ', len(train_set))\n",
    "print('Length of Validation set: ', len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Y-DuTMPxezF4",
    "outputId": "d480e410-955f-460f-f29a-193c119f54f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train tagged set:  95971\n",
      "Length of Validation tagged set:  4705\n"
     ]
    }
   ],
   "source": [
    "# Convert the nested list of (word, tag) to a single list of (word, tag) for train and validation set\n",
    "\n",
    "train_tagged_words = [train_pair for sent in train_set for train_pair in sent]\n",
    "validation_tagged_words = [validation_pair for sent in validation_set for validation_pair in sent]\n",
    "\n",
    "print('Length of Train tagged set: ', len(train_tagged_words))\n",
    "print('Length of Validation tagged set: ', len(validation_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jeuGM4oNezGB",
    "outputId": "f3051214-0746-441d-d053-23000144118d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  12076\n",
      "Number of unique tags:  12\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the number of unique words and tags in train set\n",
    "\n",
    "vocab = set([tup[0] for tup in train_tagged_words])\n",
    "tags = set([tup[1] for tup in train_tagged_words])\n",
    "\n",
    "print('Number of unique words: ', len(vocab))\n",
    "print('Number of unique tags: ', len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "PhERGTU3ezGJ",
    "outputId": "4e8482f8-8f18-4048-ac5d-f5b30d434a21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at the tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "Nqe7T6PteYkJ",
    "outputId": "093ffbb0-7306-41ab-c044-a01b1334b6f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a71ef110f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the count of each tag in the train set\n",
    "sns.countplot([tup[1] for tup in train_tagged_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQG9Hg2-KYt1"
   },
   "source": [
    "From the above graph it is evident that NOUN is the most common tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhX7WzNfezGV"
   },
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FdLyOdaezGX"
   },
   "source": [
    "#### POS Tagging Algorithm - HMM\n",
    "\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n",
    "\n",
    "In other words, to every word w, assign the tag t that maximises the likelihood P(t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to compute P(w/t) and P(t).\n",
    "\n",
    "\n",
    "P(w/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n",
    "\n",
    "P(w/t) = count(w, t) / count(t). \n",
    "\n",
    "\n",
    "The term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n",
    "\n",
    "Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P(t) and store them in two large matrices. The matrix of P(w/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EK6rzHCEezGZ"
   },
   "source": [
    "#### Emission Probabilities\n",
    "The function word_given_tag calculates the number of times the tag appears in the train set and the number of times the particular word for that tag appears. <br>\n",
    "P(w/t) = count(w, t) / count(t)<br>\n",
    "For calculating the above probability the function returns the count(w,t) and count(t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfzS4auRezGa"
   },
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    tag_count = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    w_given_tag_count = len(w_given_tag_list)\n",
    "    \n",
    "    return (w_given_tag_count, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "shvtUckcezGg",
    "outputId": "65d18f8d-fc24-4cb6-e680-2873607ccd52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " large\n",
      "(28, 6112)\n",
      "(0, 12922)\n",
      "(0, 27543) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "# large\n",
    "print(\"\\n\", \"large\")\n",
    "print(word_given_tag('large', 'ADJ'))\n",
    "print(word_given_tag('large', 'VERB'))\n",
    "print(word_given_tag('large', 'NOUN'), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vj4DS2zJezGn"
   },
   "source": [
    "#### Transition Probabilities\n",
    "The below function workinig:\n",
    "1. Save the sequence of tags that appear in the train set i.e. in the tags list\n",
    "2. Count the number of times the tag t1 appears in the train set i.e. t1_count\n",
    "3. Count the number of times the tag t2 has appeared after tag t1 in the train set i.e. t2_t1_count\n",
    "4. Return the t2_t1_count and t1_count \n",
    "\n",
    "From the above values we can calculate the transition probabilities i.e. how likely the tag t2 will appear after the tag t1 <br>\n",
    "i.e. t2_t1_count / t1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVWvCyZaezGp"
   },
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    t1_count = len([t for t in tags if t==t1])\n",
    "    t2_t1_count = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1]==t2:\n",
    "            t2_t1_count += 1\n",
    "    \n",
    "    return (t2_t1_count, t1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "fMejYmJaezGu",
    "outputId": "a3bba716-72f7-4fd0-c508-63b6c05f975a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6112)\n",
      "(4267, 6112)\n",
      "(5302, 8315)\n",
      "(454, 12922)\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "print(t2_given_t1(t2='PRON', t1='ADJ'))\n",
    "print(t2_given_t1('NOUN', 'ADJ'))\n",
    "print(t2_given_t1('NOUN', 'DET'))\n",
    "print(t2_given_t1('PRON', 'VERB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBG4loIsezG1"
   },
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRhnHFH2ezG6"
   },
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "id": "8zoVIJpRezHD",
    "outputId": "6b6f414d-616f-4677-ca28-5330fdf576bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>X</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>DET</th>\n",
       "      <th>NUM</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.079669</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.135537</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.119669</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.345124</td>\n",
       "      <td>0.069091</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.128926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.016810</td>\n",
       "      <td>0.264496</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.239153</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.146970</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.042806</td>\n",
       "      <td>0.012163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.026073</td>\n",
       "      <td>0.062321</td>\n",
       "      <td>0.074881</td>\n",
       "      <td>0.162639</td>\n",
       "      <td>0.056598</td>\n",
       "      <td>0.146582</td>\n",
       "      <td>0.182671</td>\n",
       "      <td>0.205087</td>\n",
       "      <td>0.053577</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.017011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.053252</td>\n",
       "      <td>0.220563</td>\n",
       "      <td>0.027525</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>0.065935</td>\n",
       "      <td>0.092201</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>0.173788</td>\n",
       "      <td>0.080237</td>\n",
       "      <td>0.058109</td>\n",
       "      <td>0.044706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.034866</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.092337</td>\n",
       "      <td>0.041762</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.071648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.322680</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>0.039898</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.017145</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.322680</td>\n",
       "      <td>0.062546</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.106466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.010124</td>\n",
       "      <td>0.249837</td>\n",
       "      <td>0.013717</td>\n",
       "      <td>0.043436</td>\n",
       "      <td>0.017962</td>\n",
       "      <td>0.019595</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.398432</td>\n",
       "      <td>0.102221</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.083932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.110277</td>\n",
       "      <td>0.217691</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>0.035134</td>\n",
       "      <td>0.092246</td>\n",
       "      <td>0.031187</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.134190</td>\n",
       "      <td>0.023061</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.066166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.637643</td>\n",
       "      <td>0.045580</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.009501</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.039206</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.022249</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.205532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.356189</td>\n",
       "      <td>0.205996</td>\n",
       "      <td>0.117839</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.035322</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.184921</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>0.033244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.054706</td>\n",
       "      <td>0.351878</td>\n",
       "      <td>0.006954</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>0.058878</td>\n",
       "      <td>0.052388</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.155772</td>\n",
       "      <td>0.121929</td>\n",
       "      <td>0.039870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.065118</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.010798</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.021106</td>\n",
       "      <td>0.017016</td>\n",
       "      <td>0.066263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADV      NOUN         X         .      PRON       ADP       PRT  \\\n",
       "ADV   0.079669  0.031736  0.022810  0.135537  0.014876  0.119669  0.014545   \n",
       "NOUN  0.016810  0.264496  0.029082  0.239153  0.004684  0.176923  0.044185   \n",
       "X     0.026073  0.062321  0.074881  0.162639  0.056598  0.146582  0.182671   \n",
       ".     0.053252  0.220563  0.027525  0.091302  0.065935  0.092201  0.002519   \n",
       "PRON  0.034866  0.211111  0.092337  0.041762  0.007280  0.022605  0.012261   \n",
       "ADP   0.013652  0.322680  0.035348  0.039898  0.069002  0.017145  0.001482   \n",
       "PRT   0.010124  0.249837  0.013717  0.043436  0.017962  0.019595  0.001960   \n",
       "VERB  0.081412  0.110277  0.217691  0.034283  0.035134  0.092246  0.031187   \n",
       "DET   0.012748  0.637643  0.045580  0.017679  0.003728  0.009501  0.000241   \n",
       "NUM   0.002968  0.356189  0.205996  0.117839  0.001484  0.035322  0.026714   \n",
       "CONJ  0.054706  0.351878  0.006954  0.036161  0.058878  0.052388  0.005100   \n",
       "ADJ   0.004581  0.698135  0.020452  0.065118  0.000654  0.079025  0.010798   \n",
       "\n",
       "          VERB       DET       NUM      CONJ       ADJ  \n",
       "ADV   0.345124  0.069091  0.031736  0.006281  0.128926  \n",
       "NOUN  0.146970  0.013252  0.009476  0.042806  0.012163  \n",
       "X     0.205087  0.053577  0.002703  0.009857  0.017011  \n",
       ".     0.089772  0.173788  0.080237  0.058109  0.044706  \n",
       "PRON  0.484291  0.009579  0.006897  0.005364  0.071648  \n",
       "ADP   0.008361  0.322680  0.062546  0.000741  0.106466  \n",
       "PRT   0.398432  0.102221  0.056499  0.002286  0.083932  \n",
       "VERB  0.169014  0.134190  0.023061  0.005340  0.066166  \n",
       "DET   0.039206  0.005532  0.022249  0.000361  0.205532  \n",
       "NUM   0.018106  0.003265  0.184921  0.013951  0.033244  \n",
       "CONJ  0.155772  0.121929  0.039870  0.000000  0.116365  \n",
       "ADJ   0.011780  0.005072  0.021106  0.017016  0.066263  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "05Hqj287c8Sv",
    "outputId": "4304f8df-1cdb-425f-86b9-051cb6aad8e5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heatmap of tags matrix\n",
    "# T(i, j) means P(tag j given tag i)\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(tags_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above headmap shows us that DET and NOUN, ADJ and NOUN, VERB and PRT, VERB and PRON shows good correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnVJ9NiMezHK"
   },
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "Let's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag/w)=P(w/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag/w).\n",
    "\n",
    "We'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n",
    "\n",
    "Note: P(tag|start) = P(tag|'.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "La5pGQLUezHM"
   },
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission = word_given_tag(words[key], tag)\n",
    "            emission_p = emission[0] / emission[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pJSw1JMezHR"
   },
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGsHCCj5SV9C"
   },
   "outputs": [],
   "source": [
    "# Create the list of words that are in the test set\n",
    "validation_words = [tup[0] for tup in validation_tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykW1846lezHT"
   },
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "\n",
    "start = time.time()\n",
    "vanilla_tagged_seq = Viterbi(validation_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7d7W5t-MezHY",
    "outputId": "1646d8a1-b0f2-41b7-e695-454f1223dd0c"
   },
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-XELgmO9ezHd",
    "outputId": "3fd1c599-a068-4d64-acb5-d56bf44a6fe6"
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(vanilla_tagged_seq, validation_tagged_words) if i == j]\n",
    "vanilla_accuracy = len(check)/len(vanilla_tagged_seq)\n",
    "round(vanilla_accuracy, 4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZFu_wMkezHv"
   },
   "outputs": [],
   "source": [
    "# Find all the incorrect tagged words\n",
    "vanilla_incorrect_tagged_cases = [[validation_tagged_words[i-1],j] for i, j in enumerate(zip(vanilla_tagged_seq, validation_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kQ5QXhUezHz",
    "outputId": "5710faf9-22ba-4e03-db0a-c966da417a86"
   },
   "outputs": [],
   "source": [
    "# Print the incorrect tagged words\n",
    "vanilla_incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqWa7K9m0yaP"
   },
   "source": [
    "Let's take a look at few of the words that were incorrectly tagged with the vanilla viterbi algorithm. <br>\n",
    "1. [('index', 'NOUN'), (('3436.58', 'X'), ('3436.58', 'NUM'))] - Numbers are tagged to X instead of NUM\n",
    "2. [('*', 'X'), (('attracting', 'X'), ('attracting', 'VERB'))] - attracting is tagged as X instead of VERB\n",
    "3. [('to', 'PRT'), (('locally', 'X'), ('locally', 'ADV'))] - locally is tagged as X instead of ADV \n",
    "4. [('``', '.'), (('deplorable', 'X'), ('deplorable', 'ADJ'))] - deplorable as X instead of ADJ\n",
    "\n",
    "and so on.\n",
    "\n",
    "There are many words that are incorrectly tagged which can be tagged easily using a regex tagger using the common suffixes such as -ing for VERB, -able for ADJ, and so on.\n",
    "\n",
    "Identifying numbers can also be easily done using regex which were incorrectly tagged using the vanilla viterbi algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6m_0FPeezIC"
   },
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfydFX5Pphxl"
   },
   "source": [
    "#### 1. Using RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3OIuZDixXW5"
   },
   "outputs": [],
   "source": [
    "# Function that takes a word as input and returns the tag based on the patterns using regular expression\n",
    "# If none of the patterns match then tag that word as NOUN since it is the most common tag in the corpus\n",
    "\n",
    "def regex(word):\n",
    "    # specify patterns for tagging\n",
    "    patterns = [\n",
    "      (r'\\d+.?\\d*', 'NUM'),\n",
    "      (r'\\d+-\\d+', 'NUM'),\n",
    "      (r'\\d+(st|nd|rd)', 'NUM'),\n",
    "      (r'\\*.*\\d{1,3}', 'X'),\n",
    "      (r'(,|\\.|\\'\\'|--|``|:|\\?|-LRB-|-RRB-|#|;|@)', '.'),\n",
    "      (r'.*(ible|able|al|ful|ic|ive|ous|less)$', 'ADJ'),\n",
    "      (r'.*(ward|ly|wise|wards)$', 'ADV'),\n",
    "      (r'.*(ate|ing|en|ify|ise|ize|ed)$', 'VERB'),\n",
    "      (r'.*(sion|tion|xion|ship|ry|ness|ment|ity|ty|ist|ism|hood|er|or|ee|dom|ance|ence|al|age|s)$', 'NOUN'),\n",
    "      (r'.*', 'NOUN')\n",
    "      ]\n",
    "    \n",
    "    regex_tagger = nltk.RegexpTagger(patterns)\n",
    "    tag = regex_tagger.tag([word])\n",
    "    return tag[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5726STeJ5FFm"
   },
   "source": [
    "The vanilla viterbi algorithms fails when the word is not present in the trained set that is to be tagged. This is due to fact that emission probability of the word is zero.\n",
    "\n",
    "To solve this issue we will use a RegexpTagger to identify whether a word is VERB, NOUN, NUM, and so on.\n",
    "\n",
    "The below algorithm is modified by checking whether the state_probability calculated for all the tag results in zero or not i.e. if the word is not present in the train set then the emission probability is zero that would result in zero for all the tags while calculating the state_probability.\n",
    "\n",
    "Hence check if the state_probability for all the tag is zero (i.e. p would contain only 0 for all the tags. Therefore check if sum(p) is zero or not) then call the 'regex' function defined above to tag the word based on the patterns identified; or else if the word is present in the train set then emission probability will not result in zero for atleast one tag and hence tag the word based on the most likely tag suited for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9AgXj6kezIE"
   },
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi_with_regex(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission = word_given_tag(words[key], tag)\n",
    "            emission_p = emission[0] / emission[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "        \n",
    "        # Modifying the algorithm:\n",
    "        # If the word is not present in the train set then the state_probability\n",
    "        # for all the tags will be zero and hence the p list will contains only \n",
    "        # zero for all tags.\n",
    "        # Check whether the state_probability is zero for all the tags for not i.e. word not present\n",
    "        # If the sum of p is not zero i.e. the word is present in the train set\n",
    "        # then tag the word based on the most likely tag for the word\n",
    "        if sum(p) != 0: \n",
    "            pmax = max(p)\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "          \n",
    "        # else if the word is not present in the corpus then use the RegexpTagger to\n",
    "        # tag the word. For this all the previously made function 'regex'\n",
    "        else: \n",
    "            state.append(regex(word))\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMOoKyyzezIN"
   },
   "source": [
    "#### Evaluating tagging accuracy of Viterbi algorithm modified using RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9PHOaPpezII"
   },
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "\n",
    "start = time.time()\n",
    "regex_tagged_seq = Viterbi_with_regex(validation_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_V989srksOvP",
    "outputId": "81fad70f-076b-458d-e7d0-46d8415074cf"
   },
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-MGjz2insOvk",
    "outputId": "d3388191-f6a3-4e5a-a723-fef99ee20e9c"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "check = [i for i, j in zip(regex_tagged_seq, validation_tagged_words) if i == j]\n",
    "regex_accuracy = len(check)/len(regex_tagged_seq)\n",
    "round(regex_accuracy, 4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eF1qjqufsOvy"
   },
   "outputs": [],
   "source": [
    "# Find all the incorrect tagged words\n",
    "regex_incorrect_tagged_cases = [[validation_tagged_words[i-1],j] for i, j in enumerate(zip(regex_tagged_seq, validation_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pOi-S9oRsOv3",
    "outputId": "45c55fea-cc85-4302-f0da-624015a70218"
   },
   "outputs": [],
   "source": [
    "# Print the incorrect tagged words\n",
    "regex_incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O60hFF63ezIQ"
   },
   "source": [
    "#### 2. Using lexicon based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g53QZT5cp2jj"
   },
   "outputs": [],
   "source": [
    "# The below function given a word uses the BigramTagger first to tag the word\n",
    "# If that fails then it is backoffed to UnigramTagger and even if that fails\n",
    "# then it is backoffed to DefaultTagger that tags the word as NOUN since that\n",
    "# is the most common tag in the corpus.\n",
    "\n",
    "# The function returns the tag for the given word.\n",
    "def lexicon(word):\n",
    "    default = nltk.DefaultTagger('NOUN')\n",
    "    unigram = nltk.UnigramTagger(train_set, backoff=default)\n",
    "    bigram = nltk.BigramTagger(train_set, backoff=unigram)\n",
    "\n",
    "    tag = bigram.tag([word])\n",
    "    return tag[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7R7ztFN2kZI3"
   },
   "source": [
    "Another approach to solve same issue we will use a lexicon tagger to identify whether a word is VERB, NOUN, NUM, and so on.\n",
    "\n",
    "The below algorithm is modified by checking whether the state_probability calculated for all the tag results in zero or not i.e. if the word is not present in the train set then the emission probability is zero that would result in zero for all the tags while calculating the state_probability.\n",
    "\n",
    "Hence check if the state_probability for all the tag is zero (i.e. p would contain only 0 for all the tags. Therefore check if sum(p) is zero or not) then call the 'lexicon' function defined above to tag the word based on the patterns identified; or else if the word is present in the train set then emission probability will not result in zero for atleast one tag and hence tag the word based on the most likely tag suited for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-C4bh6gp2ft"
   },
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi_with_lexicon(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission = word_given_tag(words[key], tag)\n",
    "            emission_p = emission[0] / emission[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "\n",
    "        # Modifying the algorithm:\n",
    "        # If the word is not present in the train set then the state_probability\n",
    "        # for all the tags will be zero and hence the p list will contains only \n",
    "        # zero for all tags.\n",
    "        # Check whether the state_probability is zero for all the tags for not i.e. word not present\n",
    "        # If the sum of p is not zero i.e. the word is present in the train set\n",
    "        # then tag the word based on the most likely tag for the word\n",
    "        if sum(p) != 0:\n",
    "            pmax = max(p)\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "          \n",
    "        # else if the word is not present in the corpus then use the 'lexicon' function\n",
    "        # to tag the word.\n",
    "        else:\n",
    "            state.append(lexicon(word))\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6No0k4lv4If"
   },
   "source": [
    "#### Evaluating tagging accuracy of Viterbi algorithm modified using Lexicon Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWFIHfW4v4Ip"
   },
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "\n",
    "start = time.time()\n",
    "lexicon_tagged_seq = Viterbi_with_lexicon(validation_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "x6KlxFYnv4I7",
    "outputId": "4ab8c888-46d6-4be1-80a7-d21fa0d6f79a"
   },
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bUJJpXSiv4JL",
    "outputId": "214505df-c1fe-4dcd-9bb9-0e64fa623bf5"
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(lexicon_tagged_seq, validation_tagged_words) if i == j]\n",
    "lexicon_accuracy = len(check)/len(lexicon_tagged_seq)\n",
    "round(lexicon_accuracy, 4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlT9lSrcv4JX"
   },
   "outputs": [],
   "source": [
    "# Find all the incorrect tagged words\n",
    "lexicon_incorrect_tagged_cases = [[validation_tagged_words[i-1],j] for i, j in enumerate(zip(lexicon_tagged_seq, validation_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nuWbMdpqv4Je",
    "outputId": "7068ff0b-a9e0-4ced-add3-9916cc0ab4c9"
   },
   "outputs": [],
   "source": [
    "# Print the incorrect tagged words\n",
    "lexicon_incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctlnpkHq-pN6"
   },
   "source": [
    "#### 3. Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DIqkWH9_euR"
   },
   "source": [
    "If the word is not present in the training corpus then while calculating the emission probability would result in zero.\n",
    "\n",
    "Adding 0.0001 to the emission_p would help to overcome this problem. Adding such a small value wouldn't affect the overall performance of the algorithm. Also, as this value is getting added to all the tags for all the words while the emission_p is getting calculated we can expect a decent performance for all the words without significantly affecting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2HHRvsCoGQj"
   },
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi_with_laplace(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission = word_given_tag(words[key], tag)\n",
    "            emission_p = emission[0] / emission[1]\n",
    "            \n",
    "            # While calculating the state_probability\n",
    "            # Add 0.0001 as the smoothing value\n",
    "            state_probability = (emission_p + 0.0001) * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-POSWi7JBgvf"
   },
   "source": [
    "#### Evaluating tagging accuracy of Viterbi algorithm modified using Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgtoqZ7MwvE7"
   },
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "\n",
    "start = time.time()\n",
    "laplace_tagged_seq = Viterbi_with_laplace(validation_words)\n",
    "end = time.time()\n",
    "difference = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9sbjvw53wvFT",
    "outputId": "c67db073-95c8-4f25-f8af-7ee65b67d982"
   },
   "outputs": [],
   "source": [
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bWv0fL6owvFj",
    "outputId": "6cd3cb91-7fba-4b35-e657-ea8d47d367c5"
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "check = [i for i, j in zip(laplace_tagged_seq, validation_tagged_words) if i == j]\n",
    "laplace_accuracy = len(check)/len(laplace_tagged_seq)\n",
    "round(laplace_accuracy, 4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GfooQyLwvFu"
   },
   "outputs": [],
   "source": [
    "# Find all the incorrect tagged words\n",
    "laplace_incorrect_tagged_cases = [[validation_tagged_words[i-1],j] for i, j in enumerate(zip(laplace_tagged_seq, validation_tagged_words)) if j[0]!=j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TSdb2n7nwvF0",
    "outputId": "37644af5-305e-46e4-b2e2-ec68c2fbf235"
   },
   "outputs": [],
   "source": [
    "# Print the incorrect tagged words\n",
    "laplace_incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hs9PaiTJezIT"
   },
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "N0ABpA1PezIU",
    "outputId": "4c082696-c331-4859-eb7e-52352abeeb2a"
   },
   "outputs": [],
   "source": [
    "# Comparing the accuracy of the regex and lexicon based Viterbi Model with the Vanilla Viterbi Model\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=['vanilla_accuracy', 'regex_accuracy', 'lexicon_accuracy', 'laplace_accuracy'], y=[vanilla_accuracy, regex_accuracy, lexicon_accuracy, laplace_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "z3mor2esezIX",
    "outputId": "eb9827ec-d3ab-44b8-83b2-3493b90d39a8"
   },
   "outputs": [],
   "source": [
    "print('vanilla_accuracy: {0}\\nregex_accuracy: {1}\\nlexicon_accuracy: {2}\\nlaplace_accuracy: {3}'.format(round(vanilla_accuracy, 4)*100, \n",
    "                                                                                                         round(regex_accuracy, 4)*100,\n",
    "                                                                                                         round(lexicon_accuracy, 4)*100,\n",
    "                                                                                                         round(laplace_accuracy, 4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-iSnV1sezIc"
   },
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SffL9F2AvhQf"
   },
   "source": [
    "#### 1. Vanilla Viterbi vs Viterbi Algorithm with RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VM9hgaD-rW8B",
    "outputId": "1f6c7fa8-2564-48a8-8bbb-f08eb840f743"
   },
   "outputs": [],
   "source": [
    "# Comparing the cases where the words in the vanilla viterbi algorithm got incorrectly tagged\n",
    "# while using the viterbi algorithm with RegexpTagger for the unknown words got correctly tagged\n",
    "\n",
    "# Output is in the format:\n",
    "# ((vanilla viterbi), (viterbi with regexptagger), (correct test tagged words))\n",
    "\n",
    "[j for i, j in enumerate(zip(vanilla_tagged_seq, regex_tagged_seq, validation_tagged_words)) if j[0]!=j[1] and j[1]==j[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KyIjB9EvuEd"
   },
   "source": [
    "#### 2. Vanilla Viterbi vs Viterbi Algorithm with Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KGoMBqBqvtYU",
    "outputId": "ef1b6336-2a7f-442b-8214-37a8c2eeb668"
   },
   "outputs": [],
   "source": [
    "# Comparing the cases where the words in the vanilla viterbi algorithm got incorrectly tagged\n",
    "# while using the viterbi algorithm with lexicon for the unknown words got correctly tagged\n",
    "\n",
    "# Output is in the format:\n",
    "# ((vanilla viterbi), (viterbi with lexicon), (correct test tagged words))\n",
    "\n",
    "[j for i, j in enumerate(zip(vanilla_tagged_seq, lexicon_tagged_seq, validation_tagged_words)) if j[0]!=j[1] and j[1]==j[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC8U51UFAwb8"
   },
   "source": [
    "#### 3. Vanilla Viterbi vs Viterbi Algorithm with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L_iopiTmAwcJ",
    "outputId": "493d3cfd-885c-4683-dcb8-7391fcb007b4"
   },
   "outputs": [],
   "source": [
    "# Comparing the cases where the words in the vanilla viterbi algorithm got incorrectly tagged\n",
    "# while using the viterbi algorithm with Laplace Smoothing for the unknown words got correctly tagged\n",
    "\n",
    "# Output is in the format:\n",
    "# ((vanilla viterbi), (viterbi with laplace smoothing), (correct test tagged words))\n",
    "\n",
    "[j for i, j in enumerate(zip(vanilla_tagged_seq, laplace_tagged_seq, validation_tagged_words)) if j[0]!=j[1] and j[1]==j[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi Algorithm fails to tag words that were not present in the training corpus since the emission probabilities for those words are zero.\n",
    "\n",
    "To overcome this problem we modified the Viterbi Algorithm using three methonds:\n",
    "1. RegexpTagger\n",
    "2. Lexicon based\n",
    "3. Laplace Smoothing\n",
    "\n",
    "Out of the three Regex based showed the most improvement as we can identify words using the common suffixes for different tags but it still fails to tag words based on the context. After regex based viterbi algorithm lexicon performance was better than vanilla viterbi algorithm. In the end laplace smoothing showed some improvement but it was very very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QTj6q_uKVpag"
   ],
   "name": "Assignment_POS tagging using modified Viterbi.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
